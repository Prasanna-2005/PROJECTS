{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "### Housing Price Prediction\n",
    "\n",
    "The training dataset contains three examples with four features (size, bedrooms, floors, and age).\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms | Number of Floors | Age of Home | Price (1000s dollars) |\n",
    "|-------------|--------------------|------------------|-------------|-----------------------|\n",
    "| 2104        | 5                  | 1                | 45          | 460                   |\n",
    "| 1416        | 3                  | 2                | 40          | 232                   |\n",
    "| 852         | 2                  | 1                | 35          | 178                   |\n",
    "\n",
    "We will build a linear regression model using these values so you can then predict the price for other houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 features and 3 samples\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.array([[2104.       ,    5.       ,    1.       ,   45.       ],\n",
    " [2031.57894737,    4.78947368,    1.10526316,   44.47368421],\n",
    " [1959.15789474,    4.57894737,    1.21052632,   43.94736842],\n",
    " [1886.73684211,    4.36842105,    1.31578947,   43.42105263],\n",
    " [1814.31578947,    4.15789474,    1.42105263,   42.89473684],\n",
    " [1741.89473684,    3.94736842,    1.52631579,   42.36842105],\n",
    " [1669.47368421,    3.73684211,    1.63157895,   41.84210526],\n",
    " [1597.05263158,    3.52631579,    1.73684211,   41.31578947],\n",
    " [1524.63157895,    3.31578947,    1.84210526,   40.78947368],\n",
    " [1452.21052632,    3.10526316,    1.94736842,   40.26315789],\n",
    " [1386.31578947,    2.94736842,    1.94736842,   39.73684211],\n",
    " [1326.94736842,    2.84210526,    1.84210526,   39.21052632],\n",
    " [1267.57894737,    2.73684211,    1.73684211,   38.68421053],\n",
    " [1208.21052632,    2.63157895,    1.63157895,   38.15789474],\n",
    " [1148.84210526,    2.52631579,    1.52631579,   37.63157895],\n",
    " [1089.47368421,    2.42105263,    1.42105263,   37.10526316],\n",
    " [1030.10526316,    2.31578947,    1.31578947,   36.57894737],\n",
    " [ 970.73684211,    2.21052632,    1.21052632,   36.05263158],\n",
    " [ 911.36842105,    2.10526316,    1.10526316,   35.52631579],\n",
    " [ 852.       ,    2.       ,    1.       ,   35.       ]]\n",
    ")\n",
    "y_train = np.array([460.        , 436.        , 412.        , 388.        ,\n",
    " 364.        , 340.        , 316.        , 292.        ,\n",
    " 268.        , 244.        , 229.15789474, 223.47368421,\n",
    " 217.78947368, 212.10526316, 206.42105263, 200.73684211,\n",
    " 195.05263158, 189.36842105, 183.68421053, 178.        ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling and Feature enginerring\n",
    "\n",
    "Explore feature engineering and polynomial regression which allows you to use the machinery of linear regression to fit very complicated, even very non-linear functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    computes  X, zcore normalized by column\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))     : input data, m examples, n features\n",
    "      \n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): input normalized by column\n",
    "      mu (ndarray (n,))     : mean of each feature\n",
    "      sigma (ndarray (n,))  : standard deviation of each feature\n",
    "    \"\"\"\n",
    "    # find the mean of each column/feature\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    std_dev  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_normalized = (X - mu) / std_dev    \n",
    "\n",
    "    return X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak to Peak range by column in Raw        X:[1.2520000e+03 3.0000000e+00 9.4736842e-01 1.0000000e+01]\n",
      "Peak to Peak range by column in Normalized X:[3.29101537 3.25051591 3.13339781 3.29501789]\n"
     ]
    }
   ],
   "source": [
    "X_norm = zscore_normalize_features(X_train)\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering is all about creating new features from existing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_feature_engn = np.c_[X_norm, X_norm[:,0]**2, X_norm[:,1]**2, X_norm[:,2]**2, X_norm[:,3]**2]\n",
    "# these are the new features that we have created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_x_wb(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b     \n",
    "    return p   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0\n",
    "\n",
    "    for i in range(m):                             \n",
    "        \n",
    "        err = f_x_wb(X[i],w,b) - y[i]  # error for the i-th example\n",
    "\n",
    "        dj_db += err\n",
    "        dj_dw += (err * X[i])\n",
    "                           \n",
    "        \n",
    "    return dj_db/m , dj_dw/m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, w_in, b_in, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    \n",
    "    w = w_in \n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = gradient_function(X, y, w, b)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "              \n",
    "    return w, b #return final w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b,w found by gradient descent: 176.77 , [ 26.22672059  15.824422    87.55045871  30.57785328  45.32979114\n",
      "  33.02557268 -29.07332073  50.06504874] \n"
     ]
    }
   ],
   "source": [
    "# initial_w = np.zeros(len(X_train[0]))\n",
    "initial_w = np.zeros(len(X_train_feature_engn[0]))\n",
    "initial_b = 0\n",
    "\n",
    "iterations = 100000\n",
    "alpha = 1.0e-4\n",
    "\n",
    "w_final, b_final = train(X_train_feature_engn, y_train, initial_w, initial_b, compute_gradient, alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f} , {w_final} \")\n",
    "\n",
    "# w_final, b_final = train(X_norm, y_train, initial_w, initial_b, compute_gradient, alpha, iterations)\n",
    "# print(f\"b,w found by gradient descent: {b_final:0.2f} , {w_final} \")\n",
    "\n",
    "# w_final, b_final = train(X_train, y_train, initial_w, initial_b, compute_gradient,alpha, iterations)\n",
    "# print(f\"b,w found by gradient descent: {b_final:0.2f} , {w_final} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Z-Norm\n",
    "iterations = 100000\n",
    "alpha = 1.0e-4\n",
    "b,w found by gradient descent: 277.78 , [ 28.34313317  30.29060682 -19.94524968  27.39362005] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECKING THE ACCURACY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 480.95, target value: 460.0\n",
      "Prediction: 443.90, target value: 436.0\n",
      "Prediction: 409.52, target value: 412.0\n",
      "Prediction: 377.84, target value: 388.0\n",
      "Prediction: 348.83, target value: 364.0\n",
      "Prediction: 322.52, target value: 340.0\n",
      "Prediction: 298.89, target value: 316.0\n",
      "Prediction: 277.94, target value: 292.0\n",
      "Prediction: 259.68, target value: 268.0\n",
      "Prediction: 244.11, target value: 244.0\n",
      "Prediction: 235.67, target value: 229.15789474\n",
      "Prediction: 231.62, target value: 223.47368421\n",
      "Prediction: 226.60, target value: 217.78947368\n",
      "Prediction: 220.61, target value: 212.10526316\n",
      "Prediction: 213.65, target value: 206.42105263\n",
      "Prediction: 205.73, target value: 200.73684211\n",
      "Prediction: 196.83, target value: 195.05263158\n",
      "Prediction: 186.96, target value: 189.36842105\n",
      "Prediction: 176.13, target value: 183.68421053\n",
      "Prediction: 164.32, target value: 178.0\n"
     ]
    }
   ],
   "source": [
    "m = X_train.shape[0]\n",
    "\n",
    "for i in range(m):\n",
    "    # print(f\"Prediction: {f_x_wb(X_train[i],w_final,b_final):0.2f}, target value: {y_train[i]}\")\n",
    "    # print(f\"Prediction: {f_x_wb(X_norm[i],w_final,b_final):0.2f}, target value: {y_train[i]}\")\n",
    "    print(f\"Prediction: {f_x_wb(X_train_feature_engn[i],w_final,b_final):0.2f}, target value: {y_train[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
